---
title: "Landslides"
author: "Mahlon Scott"
date: "2025-05-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(jsonlite)
library(ZINB.GP)
library(sf)
library(ggplot2)
```

## Oregon Landslide Dataset

We'll use a sample dataset taken from the state of oregon, this dataset consists 
of the locations of historical landslides occuring starting at 1928 and continuing 
up to 2023, though the data is much denser in recent years as data collection 
has increased. The raw dataset can be found here: https://arcg.is/OCST5. 

### Simple Plots

The most obvious useful covariate for landslide prediction is the variability in 
elevation in an area, as areas that have large cliffs are naturally more likely to 
have landslides occur, this relationship can be seen in the plots below.

```{python, echo=FALSE}
import geopandas as gpd
import matplotlib.pyplot as plt
elev = gpd.read_file("elev.geojson")
landslides = gpd.read_file("landslidesKnownTime.geojson").to_crs("EPSG:4326")
_ = elev.plot(column="elev_std", legend=True)
_ = plt.title("Std. Dev of Elevation, 30 arcsec grid")
_ = plt.xlabel("Longitude")
_ = plt.ylabel("Latitude")
plt.show()
```
```{python, echo=FALSE}
_ = landslides.plot(alpha=0.5)
_ = plt.title("Historical Oregon Landslide Locations")
_ = plt.xlabel("Longitude")
_ = plt.ylabel("Latitude")
plt.show()
```
Grouping the data by county, we can apply a zero-inflated negative binomial model 
to model the landslide frequency in each county. We then have an an s by t matrix, 
where s is the number of counties, t is the number of years in the dataset, and 
each entry is a nonnegative integer. The spatiotemporal random effects in the 
ZINB_NNGP model will account for some of the additional spatial variability not 
explained just by the standard deviation of the elevation in a region, as well 
as some of the recording frequency variability.

### Preprocessing the Dataset
```{r}
observations <- read.table("landslides_county_year.dat")
observations
```
From this observation matrix, we can generate our response vector y, and the 
spatial and temporal design matrices Vs and Vt.
```{r}
# Call the helper function for creating the design matrices from the observation 
# matrix
out <- make_y_Vs_Vt(observations)
Vs <- out$Vs
Vt <- out$Vt
y <- out$y

# Show quick example of contents of temporal design matrix, the first 6 
# observations should correspond to the first time in this dataset.
head(Vt)
```

We also need the spatial and temporal distance matrices between each pair of 
positions in the dataset, aligned with the observations table above. Note that 
these matrices should be scaled reasonably such that the maximum value in them 
will not cause stability issues in the RBF kernel, values less than about 3 should 
be fine. 

An example of one way to perform this scaling will be shown for the temporal 
design matrix below, which is to just do a min-max scaling. 
For predictions at new locations, this scaling factor must be 
saved to scale those in the same way.
```{r}
# Temporal design matrix
Dt <- as.matrix(unname(read.table("time_dist.dat")))
head(Dt)
```
```{r}
scale_t <- 0.0001 * max(Dt) # Set a reasonable scale for the time matrix
Dt <- Dt / scale_t
head(Dt)

# Spatial design matrix, set a reasonable scale here as well
Ds <- as.matrix(unname(read.table("county_dist_scale.dat"))) * 100
```

Then, we need our covariates i.e. the std. dev of elevation (we average this 
over the entire county in this case to provide a true county-level summary), 
though the model can accept observation-specific predictors as well. 

Since we conveniently have provided the spatial/temporal design matrices, one 
can just multiply any spatial covariates by the spatial design, and temporal covariates 
by the temporal design to get the required format for the covariates X.
```{r}
# Provide elev data on a county-by-county basis
countyData <- read.csv("counties.csv")
X <- Vs %*% countyData$county_elev_std[2:length(countyData$county_elev_std)] # Note this does ignore the covariate at the intercept

# Add an intercept term to X
X <- cbind(1, X)
```

Finally, with our data preprocessed, we just need the spatial coordinates for the 
spatial NNGP approximation, and we can run the model. These must also be scaled 
for use in Gaussian kernels, for analyses over larger areas, ensure to scale in 
a way that minimally distorts distances between point.
```{r}
countyCoords <- as.matrix(unname(read.table("counties_coords.dat")))
head(countyCoords)
```

### Results

Now, let's run the model. This is how you would do it for our dataset.
```{r, eval=FALSE}
# Set a prior for the temporal length scale, this ensures that it will be accepted somewhat frequently
# Note that this sampler needs to be run for much longer, more like 300000 iterations to get reasonable convergence
# on this dataset, this is just for demonstration.
ltprior <- list(max = 16, mh_sd = 0.5, a = 1, b = 0.001)
output <- ZINB_GP(X, y, countyCoords, Vs, Vt, Ds, Dt, nsim=20000, burn=2000, print_progress = TRUE, thin=12, save_ypred = TRUE)
```

We've cached the output of a much longer run in a file, let's load that instead of actually 
running the model.
```{r}
# Loads the output variable from the model.
load("outMat.rda")
```

Now, we can show some example analyses.
Plot typical values of model coefficients.
```{r}
Beta <- output$Beta
boxplot(Beta[,2], main="Elev_STD Coefficient, NB model")
boxplot(Beta[,1], main="Intercept, NB model")
```
```{r}
Alpha <- output$Alpha
boxplot(Alpha[,2], main="Elev_STD Coefficient, Logit model")
boxplot(Alpha[,1], main="Intercept, Logit model")
```

95% CIs for model coefficients, no coefficients are significant in the NB model, the spatial/temporal effects seem to dominate.
```{r}
cat("Intercept CI (NB): \n")
quantile(Beta[,1], c(0.025, 0.975))
cat("\nElev_STD CI (NB): \n")
quantile(Beta[,2], c(0.025, 0.975))
```

Now, let's look at the random effects.
```{r}
counties <- st_read("./orCounties.geojson")
counties$NB_effect <- c(mean(output$Beta[,1]),colMeans(output$C))
plot(counties["NB_effect"], axes=TRUE)
```

```{r}
counties$Logit_effect <- c(mean(output$Alpha[,1]),colMeans(output$A))
plot(counties["Logit_effect"], axes=TRUE)
```
We see that the Multnomah county is much more likely to be at risk of receiving landslides 
than the rest of the counties. When counties are at risk, the left side of the state is more likely to 
receive more landslides than the right side of the state.

Now, we can investigate the temporal random effects
```{r}
years <- read.csv("./Years.csv")$X0
plot(years, c(mean(output$Alpha[,1]), colMeans(output$B)), main="Temporal Effects, NB model")
```

It seems that as time goes on, the frequency of observed landslides increases, 
perhaps this is due to better recording of landslides in the modern 
era? It appears that the temporal effects for the logistic (at-risk) model are unaffected however.

```{r}
plot(years, c(mean(output$Beta[,1]), colMeans(output$D)), main="Temporal Effects, Logit model")
```
It looks like sometime in the 1990s, there were quite a lot of landslides all over 
the state. Looking at the historical record, we see that there was a large flood 
of the Willamette river valley around this time. In the observations, the counties 
with many landslides are all near the Willamette river valley, as seen below, and 
this is likely a significant contributing factor to the increase of landslides here.

Now, we can answer a potentially interesting question: If an event near the severity 
of the flood in 1996 was to occur again next year, which counties would likely 
have the most landslides, and what are likely bounds on the number of landslides 
that would occur? We can look at the at risk probability:

```{r}
logit <- \(x) log(x / (1 - x))
sigmoid <- \(x) 1 / (1 + exp(-x))

susceptAvg <- rowMeans(sigmoid(Vs %*% t(output$A) + quantile(output$B[,36], 0.90) + X %*% t(output$Alpha))) %*% Vs / 64
# Add an entry for the intercept county, 
counties$lr_suscept_avg <- c(sigmoid(quantile(output$B[,36], 0.9) + mean(output$Alpha[,1])), susceptAvg)
plot(counties["lr_suscept_avg"], main="At Risk Probability, Average, Extreme Event like 1996", axes=TRUE)
```

And the NB results:

```{r}
# Simulate results of other years like 1996
int_rows <- apply(Vs == 0, 1, all)
p <- sigmoid(Vs %*% t(output$C) + quantile(output$D[,36], 0.90) + X %*% t(output$Beta))
rands <- matrix(0, nrow=nrow(p), ncol=ncol(p))
for (i in 1:nrow(p))
{
    for(j in 1:ncol(p))
    {
        rands[i,j] <- rnbinom(n=1, size=output$R[j], prob=(1 - p[i,j]))
    }
}
avgSlides <- rowMeans((t(Vs) %*% rands) / 64)
# Add an entry for the intercept
counties$nb_res_avg <- (c(mean(apply(rands[int_rows,], 2, sum) / 64) * sigmoid(quantile(output$B[,36], 0.9) + mean(output$Alpha[,1])), avgSlides * susceptAvg))
plot(counties["nb_res_avg"], main="Posterior Mean Landslides, Event worse than 1996 Flood", axes=TRUE, breaks=c(1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 20000))
```

Since the model typically sees that the most landslides occur in Multnomah, it 
predicts that this county is at high risk if a severe 
landslide causing event, similar to the flood that caused many landslides in the 
counties below it was to occur there. 

Trace plots for this run can be found in the trace folder if interested.